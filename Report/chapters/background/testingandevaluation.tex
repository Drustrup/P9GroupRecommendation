\section{Testing and Evaluation}
This section covers the testing and evaluation methods we will use. In specific k-fold testing and our evaluation metrics will be explained.

\subsection{K-fold Testing}
Stemming from cross validation in the field of statistics, k-fold testing is a commonly used technique in regards to recommender systems. The basic principle in the method is to split the dataset into $k$ equally sized parts or folds, and then use one of the folds as the testing or validation set, while the rest of the folds are used as training data. This process is then repeated $k$ times, so each fold is used as the validation once. After it has been repeated all $k$ times, the results can then be average to end up with a single estimation. An example of how the k-fold ordering could be structured can be seen in Table \ref{tbl:bg_k-fold}. Typically either 5-fold or 10-fold testing is what is used.

\begin{table}[H]
	\centering
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		& \multicolumn{3}{l|}{Training} & \multicolumn{1}{l|}{Validation} \\ \hline
		First  & 1        & 2        & 3       & 4                               \\ \hline
		Second & 2        & 3        & 4       & 1                               \\ \hline
		Third  & 3        & 4        & 1       & 2                               \\ \hline
		Forth  & 4        & 1        & 2       & 3                               \\ \hline
	\end{tabular}
	\caption{Example of the rotation of folds in a 4-fold dataset}
	\label{tbl:bg_k-fold}
\end{table}

\subsection{Evaluation Metrics}
We will use a variety of evaluation metrics on two areas. Some are used for measuring the quality of our recommender, and the other for the precision of the aggregation.
\subsubsection{Root Mean Square Error}
The root mean square error is a common accuracy measure for recommender systems. For the algorithm, RMSE is calculated using the equation shown in \ref{eq:background_rmse}. As opposed to the other common measure, Mean Absolute Error, RMSE penalizes larger errors relatively more harshly.

\begin{equation} \label{eq:background_rmse}
\text{RMSE} = \sqrt{\frac{1}{|\tau|}\sum_{(u,i)\in \tau}(\hat{r}_{ui}-r_{ui})}
\end{equation}

\begin{equation}\label{eq:background_svd_rmse}
	\text{RMSE} = \sqrt{\frac{\sum_{u=1}^{U}\sum_{i=1}^{I}(\sum_{k=1}^{K}(A_{uk} B_{ki}) - M_{ui})^2}{UIK}}
\end{equation}

\subsubsection{Precision and Recall}
Two common evaluation metrics for recommendation systems are precision and recall. Precision and recall sees the method as a binary classifier. We can express a recommender system as a binary classifier that predicts an item as to be recommended or not recommended for each presented item. For each item, the actual classification can be either true or false, resulting in four possible outcomes which can be expressed with a confusion matrix as seen in table \ref{tab:testandevaluation_confusionmatrix}. An item can be classified as recommended and actually be a recommended item, making it a true positive. Alternatively, a non-recommended item that is in actuality not recommended is a true negative. An item that is not recommended by the classifier, but belongs with the recommended items is a false negative, and the reverse is a false positive.

\begin{table}[H]\label{tab:testandevaluation_confusionmatrix}
	\centering
	\begin{tabular}{ |l|l|l| }
		\toprule{2-3}
		& Predicted: Negative & Predicted: Positive \\ \hline
		Actual: Negative & True Negative(TN) & False Positive(FP) \\ \hline
		Actual: Positive & False Negative(FN) & True Positive(TP) \\ \hline
	\end{tabular}
	\caption{text}
\end{table}

Precision is the degree to which the method gets the correct classification out of all the positive guesses. The score is calculated by dividing the number of correct guesses, true positives, with the sum of the true positive and the incorrect guesses, false positives.

\begin{equation}\label{eq:precision}
	\text{Precision} = \frac{TP}{TP+FP}
\end{equation}

Recall is the degree of relevant elements uncovered. The score is calculated by dividing the number of true positives with the sum of true positive and false negatives as seen in \ref{eq:recall}.

\begin{equation}\label{eq:recall}
\text{Recall} = \frac{TP}{TP+FN}
\end{equation}

There are ways to skew either metric. A recommender system, which predicts a positive for only the most certain items, can likely score highly on precision as the number of false positives would be very low. However, being too conservative can result in an increase of false negatives, which reduces the recall score.

The $F_1$ score combines precision and recall into a single harmonic mean score, as to better reflect the overall performance than either metric can on their own.

\begin{equation}
	F_1 = 2 \cdot \frac{precision \cdot recall}{\text{precision} + \text{recall}}
\end{equation}