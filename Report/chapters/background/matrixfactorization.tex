\subsection{Matrix Factorization Models} \label{bg:sub:factorizationmodels}
%Function
Matrix Factorization Model methods are dimensionality reduction techniques used for recommendation that generally find latent features for items, and the propensity for users towards each latent feature.
Matrix Factorization (MF) makes an approximation of a matrix of ratings by decomposing it into multiple matrices. Since MF approximates matrices in an offline step before recommendation, it scales better compared to contemporary Collective Filtering techniques during recommendation. It performs well even when data sparsity is a concern due to the matrix approximation and the dimensionality reduction.
As data sparsity grows in a set, the less accurate every method gets. Factorization methods counteract this by compressing the data, making it less sparse, and easier to cluster.

%Cosine Similarity > Pearson Correlation (Collaborative Filtering Recommender Systems by Michael D. Ekstrand)
%Reduced coverage makes pearson/cosine nearest neighbor weak (source 2000)
\subsubsection{Singular Value Decomposition}
%Overview
Commonly referred to by its acronym, SVD, was popular during Netflix's movie recommender contest amongst the top performing entrants.

On its own, SVD is a dimensionality reduction technique. However in 2006 Simon Funk \cite{svdsimonfunk} popularized it as a recommender method with some modifications.
%Function
Given a matrix of user ratings, SVD works by decomposing the matrix as seen in \ref{eq:svd_decomp} into two matrices called the left and right singular vectors, $U$ and $V$, and a matrix with the singular values on the diagonal, $\Sigma$, as per the SVD theorem\todo{find source: Golub and Kahan 1965}. For this example, the left singular vector would hold the users on one side, and their inclination towards each latent feature. The right vector would conversely hold all the rated items, and their inclination towards each latent feature.

\begin{equation} \label{eq:svd_decomp}
\centering
M = U\times \Sigma \times V^T
\end{equation}

SVD considers as many features as there are ranks, but not all of them are influential. Reducing the number of features to consider is trivial, as the largest singular values stored in $\Sigma$ have the most influence.

At this point, it is possible to find some interesting similarities among the users, however to use SVD for recommendation, we must account for the missing ratings. By slightly tweaking the equation, as seen in \ref{eq:svd_AB_squeezing}, we get an A and B matrix, holding users and items respectively. This is results in three matrices looking like \ref{fig:svd-ab-squeeze}\todo{Fix ref to figure below}.

\begin{equation} \label{eq:svd_AB_squeezing}
	\centering
	U\times \Sigma \times V^T = U\Sigma^{1/2} \Sigma^{1/2}V^T = AB
\end{equation}

\begin{figure} [H] \label{fig:svd-ab-squeeze}
	\centering
	\includegraphics[scale=0.75]{svd_ab_squeeze.png}
	\caption{The decomposition of a rating matrix using SVD}
\end{figure}
\todo{Temporary figure from course slides - replace with own}
Having moved to the dense subspace of each rated item and user being defined by their features rather than their explicit ratings, data sparsity becomes less of a problem, as missing ratings are squeezed out.

\subsubsection{Gradient Descent}
Funk-SVD extends the knowledge of the latent factors from the known ratings onto the unknown ratings, however doing so requires the A and B matrices to be adjusted to minimize the error in the approximation of the rating matrix.

In Funk's blog, the gradient descent optimization algorithm is used to learn the A and B matrices to minimize the error, as seen in Equation \ref{eq:gradient_descent_error}.

\begin{equation}\label{eq:gradient_descent_error}
\text{Error} = |R-\hat{R}| = |R - AB|
\end{equation}
\todo{Not finished - needs to be expanded and properly explained}

%\begin{equation}\label{eq:rmse}
%	\text{RMSE} = \sqrt{\frac{\sum_{u=1}^{U}\sum_{i=1}^{I}(\sum_{k=1}^{K}(A_{uk} B_{ki}) - M_{ui})^2}{UIK}}
%\end{equation}

For the cost function, gradient descent moves towards the local minimum over several iterations.
\todo{Subsection still getting written}

\todo{Merits of using stochastic gradient descent}

\subsubsection{Non-negative Matrix Factorization}

Non-negative Matrix factorization is a constraint on the usual matrix factorization. As shown in \ref{eq:nmf}, given a non-negative matrix, we can find non-negative matrix factors $W$ and $H$ such that they approximate $V$.

\begin{equation} \label{eq:nmf}
	V \approx W H
\end{equation}

%http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/CFEMF_KDDW2007_4614[0].pdf

\subsubsection{Collective Matrix Factorization}
%Problem/specialcase introducing necessity of extension
%"Collective matrix factorization is a very general technique
%for revealing low-rank representations for arbitrary
%matrix collections. However, the practical applicability
%of earlier solutions has been limited since
%they implicitly assume all factors to be relevant for all
%matrices" (https://arxiv.org/pdf/1312.5921v2.pdf)
%Function
%MF is single-view
%Multi-view are many matrices
%Augmented Multi-view is CMF
%Performance vs normal MF
%???