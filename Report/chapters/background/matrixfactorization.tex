\subsection{Matrix Factorization Models} \label{bg:sub:factorizationmodels}
%Function
Matrix Factorization Models methods are dimensionality reduction techniques used for recommendation that generally find latent features for items, and the propensity for users towards each latent feature.
%Decomposing the rating matrix/Latent Semantic Indexing
%Approximating a matrix of ratings
Matrix Factorization allows for an approximation of a matrix of ratings. The standard Matrix Factorization method consists of approximating the matrix containing users and their ratings of items into multiple matrices.
%Users, u x Items, i, matrix R of rank n, approximated to two matrices of P, of size u x k, and Q, of size i x k, with k < n.
%k is the dimensional latent space.
%Performance
%Faster than both. Scales better.
Since MF approximates matrices in an offline step before recommendation, it scales better compared to contemporary Collective Filtering techniques during recommendation. It performs well even when data sparsity is a concern due to the matrix approximation and the dimensionality reduction.
As data sparsity grows in a set, the less accurate every method gets. Factorization methods counteract this by compressing the data, making it less sparse, and easier to cluster.


%Performance vs other techniques (pearson and cosine, describe other techniques)
%Scales better than nearest neighbor since our matrices are approximated (smaller)
%As noted in the 2011 SysRec book - neighborhood methods have 2 flaws: Limited coverage, Sensitivity to sparse data. MF counteracts this via approximation and dimensionality reduction.
%Cosine Similarity > Pearson Correlation (Collaborative Filtering Recommender Systems by Michael D. Ekstrand)
%Reduced coverage makes pearson/cosine nearest neighbor weak (source 2000)
\subsubsection{Singular Value Decomposition}
%Overview
Commonly referred to by its acronym, SVD, was popular during Netflix's movie recommender contest amongst the top performing entrants.

%Function


%Challenges
SVD has a few challenges to overcome to make it effective, but is among the most accurate methods at the time. First of all, it requires a dense matrix to function. So in the case of missing ratings, 
%http://buzzard.ups.edu/courses/2014spring/420projects/math420-UPS-spring-2014-gower-netflix-SVD.pdf

\note{Funk SVD is about sparse matrices - look at slides - using stochastic descent}

\subsubsection{Nonnegative Matrix Factorization}

%http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/CFEMF_KDDW2007_4614[0].pdf

\subsubsection{Collective Matrix Factorization}
%Problem/specialcase introducing necessity of extension
As an extension to the MF method, Collective Matrix Factorization adds on more matrices.
%"Collective matrix factorization is a very general technique
%for revealing low-rank representations for arbitrary
%matrix collections. However, the practical applicability
%of earlier solutions has been limited since
%they implicitly assume all factors to be relevant for all
%matrices" (https://arxiv.org/pdf/1312.5921v2.pdf)
%Function
%MF is single-view
%Multi-view are many matrices
%Augmented Multi-view is CMF
%Performance vs normal MF
%???