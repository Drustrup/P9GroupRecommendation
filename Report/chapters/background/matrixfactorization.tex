\subsection{Matrix Factorization Models} \label{bg:sub:factorizationmodels}
%Function
Matrix Factorization Model methods are dimensionality reduction techniques used for recommendation that generally find latent features for items, and the propensity for users towards each latent feature.
Matrix Factorization makes an approximation of a matrix of ratings by decomposing it into multiple matrices.
Since MF approximates matrices in an offline step before recommendation, it scales better compared to contemporary Collective Filtering techniques during recommendation. It performs well even when data sparsity is a concern due to the matrix approximation and the dimensionality reduction.
As data sparsity grows in a set, the less accurate every method gets. Factorization methods counteract this by compressing the data, making it less sparse, and easier to cluster.

%Cosine Similarity > Pearson Correlation (Collaborative Filtering Recommender Systems by Michael D. Ekstrand)
%Reduced coverage makes pearson/cosine nearest neighbor weak (source 2000)
\subsubsection{Singular Value Decomposition}
%Overview
Commonly referred to by its acronym, SVD, was popular during Netflix's movie recommender contest amongst the top performing entrants.

On its own, SVD is a dimensionality reduction technique. However in 2006 Simon Funk\todo{ref til funk} popularized it as a recommender method with some modifications.
%Function
SVD works by for a matrix of user ratings, splitting up the matrix into two matrices called the left and right singular vectors and a matrix with the singular values on the diagonal, as per the SVD theorem\todo{find source: Golub and Kahan 1965}. For this example, the left singular vector would hold the users on one side, and their inclination towards each latent feature. The right vector would conversely hold all the rated items, and their inclination towards each latent feature.
The number of features can be limited to the largest few singular value to keep the matrices smaller and traversable without losing accuracy.
At this point, it is possible to find some interesting similarities, however to use SVD for recommendation, we must account for the missing ratings. By slightly tweaking the equation, \ref{eq:svd_AB_squeezing}, we get an A and B matrix, holding users and items respectively.

\begin{equation} \label{eq:svd_AB_squeezing}
	\centering
	M = U\times \Sigma \times V^T = U\Sigma^{1/2} \Sigma^{1/2}V^T = AB
\end{equation}

\begin{figure}
	\centering
	\includegraphics[scale=0.75]{svd_ab_squeeze.png}
\end{figure}

Having moved to the dense subspace of each rated item and user being defined by their features rather than their explicit ratings, data sparsity becomes less of a problem, as missing ratings are squeezed out.

%However this alone is not enough

%\subsubsection{Gradient Descent}

%http://buzzard.ups.edu/courses/2014spring/420projects/math420-UPS-spring-2014-gower-netflix-SVD.pdf

\subsubsection{Nonnegative Matrix Factorization}

%http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/CFEMF_KDDW2007_4614[0].pdf

\subsubsection{Collective Matrix Factorization}
%Problem/specialcase introducing necessity of extension
%"Collective matrix factorization is a very general technique
%for revealing low-rank representations for arbitrary
%matrix collections. However, the practical applicability
%of earlier solutions has been limited since
%they implicitly assume all factors to be relevant for all
%matrices" (https://arxiv.org/pdf/1312.5921v2.pdf)
%Function
%MF is single-view
%Multi-view are many matrices
%Augmented Multi-view is CMF
%Performance vs normal MF
%???