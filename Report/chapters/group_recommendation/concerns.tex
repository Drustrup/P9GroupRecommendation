\section{Concerns}
During the implementation process some concerns regarding..

\subsection{Scalability}
During the implementation scalability was not a major focus point for us and have not yet proven to be a problem with our setup. 

But a problem could occur if we were to look at a higher top-k than 10, then we could potential have a scaling problem. We have a rather high worst-case runtime on our algorithms. BC, BWC, and BEC have a worst-case on $O(n*m)$ and BTC have one on $O(n*m*v*z)$ where n is the individual items, m is the group size, v is a subset of m, z is top-k. Lets say that we have a top-k on 10 and a group size of 40 and their are no overlaps then n would be 400 items long but if top-k were 50 then n would be 2000. 

\subsection{Recommending for Ephemeral Groups}
For the sake of the developing, testing and evaluation the model-based recommendation in the form of SVD is very appropriate as we only have to train it once and then we have all the predictions. 

If the group recommendation setup were to be used in real world applications, as the virtual reality example with random users described in Section \ref{sec:introduction_scenario}, using SVD alone would be a cumbersome task due to the frequently new released items and new users, which would require us to retrain our model. A real world version should be more flexible so it would be able to handle changes to the user and item base. 

One approach that could solve this is switching to a memory-based recommeder method like pearson corralation or a nearest neighbour approach. This would make a more flexible recommender system that would be able to handle the new items and users. But although this solves the flexibility problem it present us with other problems. One of these problems is scalability with large amount of data\cite{DBLP:conf/adaptive/SchaferFHS07}. Predictions with these approaches are done online, so whenever a user needs recommendations they have to be calculated first. The calculation requires similar users to one receiving them, this requires making similarity calculations on users in a database which can be a comprehensive task on large amounts of data.

Another approach would be a combination of a model- and memory-based methods. This could as an example be using the SVD as we do now and then if a new user should appear use a nearest neighbour approach to find a similar user in the model and use its recommendations instead. This approach still leaves us with the problem with new items. 




