\subsubsection{Markov Chain}\label{sec:markovchain}
%Dwork et al propose a Markov Chain for aggregating ranked lists called \MC\cite{rank:aggregation}. \MC has a set of states, $S = \{1, 2,..., |I|\}$, corresponding to a set of all the items, $I$.

Dwork et al propose a Markov Chain for aggregating ranked lists, called \MC\cite{rank:aggregation}. \MC generalizes the heuristics of the Copeland Method, where a winner is the candidate which wins the most pairwise contests\cite{saari1996}.

\MC is a process where we note the possibility of transitioning from one state to another state over time. The \MC state space, $S$, corresponds to a set of all the items, $I$, such that $S = \{1, 2,..., |I|\}$. The transition probabilities between states are represented by a transition matrix, $P = |I| \times |I|$, covering the probability, $p_{ij}$, between any item pair $i \in I$ and $j \in I$.

To calculate the probabilities, let $c_i$ be the set of items from $I$, that for the majority of the ranked lists we aggregate for, $\tau_1, \tau_2, ...,\tau_u$, it holds that $\tau_u(i) > \tau_u(j)$. The probabilities of $P$ is found according to Equation \ref{eq:markovchain_p_ij}. $\lambda$ is a variable for teleporting, which provides a small increase in accurate. Via tuning, we found that $\lambda = 0.05$ is a good value. For the case of a missing item from either or both lists, the item is considered to be on the lowest possible rank.

\begin{equation}\label{eq:markovchain_p_ij}
p_{ij} = (\frac{|c_i|}{|I|})(1-\lambda)+(\frac{\lambda}{|I|})
\end{equation}

For the probability of state $i$ staying in state $i$, we have Equation \ref{eq:markovchain_p_ii}.

\begin{equation}\label{eq:markovchain_p_ii}
p_{ii} = (\frac{|I|-|c_i|}{|I|})(1-\lambda)+(\frac{\lambda}{|I|})
\end{equation}

When the transition matrix is calculated, the result can be found via the stationary distribution for $P$. A distribution vector is a vector of size $|I|$, which holds non-negative values, representing how the states are distributed. For an initial distribution, $x$, then $xP^t$, is the same initial distribution after $t$ steps down the chain. The stationary distribution is where the state distribution stops changing regardless of taking more steps.

For practical purposes, we can approximate the stationary distribution for $P$ via application of the power-iteration algorithm. So the approximate distribution, $r$, is found in Equation \ref{eq:markovchain_power} for a number of steps, $t$. Via tuning, we found that $t = 30$ was a good value.

\begin{equation}\label{eq:markovchain_power}
r = xP^t
\end{equation}

The result of \MC is then found as the $k$ items with the biggest shares of $r$.