\section{Introduction}
Many of the decisions we make are based on recommendations, from either people we know or recommender systems tailored to personal preferences. This can be helpful due to the high amounts of information we process in our everyday lives\cite{Edwards2001}. The recommendations, or more specifically in our case, the recommender system, can cut down the number of options to a manageable level and thereby augment the decision-making process without forcing a decision.

The problem with the traditional recommender systems is that they typically make recommendations tailored to one person but often these decisions needs to be taken in a social context. 

For some scenarios, such as for selecting a movie on a streaming service, finding a restaurant, or deciding on a vacation destination, the inclusion of a social context would change the problem from that of knowing ones own preferences to that of an entire group in the given context.

A problem regarding taking the social context into consideration is that the recommender has to strive for consensus between the people it recommends to. An already complex problem is made even harder by having to solve it for multiple users simultaneously with new rules in play. From here, we will reference to this problem as making a group recommendation.

%Different methods for group recommendations 
When making group recommendations there are two main approaches, namely profile aggregation and recommendation aggregtion \cite{profilvsrec}. The idea behind profile aggregation is to aggregate the users' preferences into a single group profile and make aggregations based on that profile. The other approach is to consider each user individually and aggregate the recommendations for the users into one aggregation that fits the groups preferences. In this paper we have chosen to focus on aggregation recommendation.

%as we want to make the method work for shifting groups and we deem this approach to fit this case best. \note{find source or fitting argument, Lukas: not sure there are any}

%Describe the approach of using top-k lists
As we are going to aggregate the users recommendations we have chosen to only focus on the top-k part of their recommendations and return a list of recommendations of size $k$ as a result. Furthermore, the top-k lists will be ranked with the highest rated item at first position on the list.

With ranked top-k lists being partial lists, we have selected four types of aggregation methods which have shown good results when used for aggregating partial lists. The methods we used were Borda Count, Markov Chain, Spearman's Footrule, and Average\cite{Masthoff2004, rank:aggregation}.

%Problem regarding evaluation of the aggregation results
For group recommendations we faced the challenge of evaluating the result without a dataset to provide a ground truth. However, from the information retrieval domain, we found measures to evaluate the quality of queries that can be used to evaluate the quality of a ranked list of recommendations and there are many datasets available for individual recommendations.

%Baltrunas
One such dataset is the 100k MovieLens dataset used by Baltrunas et al for a group recommender setup\cite{movielens100k, Baltrunas:2010:GRR:1864708.1864733}. 
They used aggregation methods such as Borda Count and Average and evaluated their performance using Normalized Discounted Cumulative Gain. Their tests were done on groups of size 2, 3, 4, and 8 and the findings they made were that the quality of the recommendation did not always drop even when the group size grew. Furthermore, their result showed a significant quality drop from group size 4 to 8. We adapted some of their approaches, more specifically Borda Count, Average, and Normalized Discounted Cumulative Gain, and setup in order to make further tests for larger groups to document if the decrease continues at the same rate as between group size 4 and 8.


%With some overlap with us in chosen methods, such as Borda Count, and measures, such as nDCG, Baltrunas et al found that the quality of the recommendation did not always drop even when the group size grows\citep{Baltrunas:2010:GRR:1864708.1864733}. However, their biggest group size was 8, and it was unclear if the trend could be seen for bigger group sizes\note{Check here, if you want to imply we're looking for this or whether the quality stops decreasing for increasing groupsizes}.

%We find out whether this is the case

\subsection{Research Questions}
Among common aggregation methods, given ranked top-k lists $\tau_1, ... , \tau_u$, where $u$ is the number of group members, which method can provide the most optimal group recommendation per measures such as satisfaction or distance from individual preferences of the group?

%Is it the case for all aggregation methods that as the number of users, $u$, increases, the quality of the recommendations decrease per our measures? For a set of groups, $G = \{ g_1, ..., g_n \}$ of corresponding group sizes, $U = \{ u_1, ..., u_n \}$, and group recommendation setup $f(x)$ that takes those groups as input and returns an evaluation of that recommendation, can we find a group size such that $f(g_a) > f(g_b)$ where $u_a > u_b$\note{Correct this part}.

Baltrunas et al supplies us with some results for the performance of a group recommender setup. Their results for group sizes 2, 3, and 4 are very close and perform well, but they show a drop off in performance for group size 4 to 8. Is it possible to reproduce similar test results and with larger groups to investigate if the drop off continues? Furthermore, is it possible to verify the results with additional measures?

%Find if derivative of f(g_1) = derivative of f(g_2), for any u_n where u_1 < u_2

\subsection{Structure of the Paper}
The structure of the paper is as follows. Section \ref{sec:systemoverview} gives a short overview of the implemented system. Section \ref{sec:grouprecommendation} describes the methods used of making a group recommendation. In Section \ref{sec:evaluation} we will present the evaluation including setup and our results. In Section \ref{sec:discussion} we discuss the results of the evaluation and in Section \ref{sec:conclusion} we will present our conclusion and future work.