\section{Discussion} \label{sec:discussion}
Across all the methods tested we saw a similar fall in nDCG scores and distance for the distance measures as group size increased, except for some methods in Rating nDCG.

We also saw a trend of the nDCG score being a good indicator for how well the same  methods performed for the distance measure. Though SF performed well in SFD, but it fell behind on other measures, with the same to be said for Avg and its performance for Rating nDCG. This stresses the importance of testing a general purpose method from many angles.

Across all the measurements, \MC is almost identical in performance to that of BC. As the underlying heuristic of the \MC method is the Copeland Method, it raises the possibility that other extensions of Markov Chain can achieve even greater results.

However, for \MC, other factors such as complexity and speed limits the utility of it compared to BC, as BC is both simple to implement and can be implemented in linear time whereas \MC is usually slower and our implementation reflected this. However optimizations for \MC and its variants exists which run in quadratic time which is a significant running time improvement\cite{rank:aggregation}.

Of all the measures, Avg has the worst results overall aside from Rating nDCG. The results from Rating nDCG in isolation sees Avg perform the best. So if Rating nDCG is a better measurement because it does not consider the rankings, and instead looks directly at the ratings given by the users, then Avg is providing the better recommendations.

However, results from Rating nDCG for Avg and the other methods are so close that there is little practical value for one method to the other. Should it be the case that Rating nDCG is the best measure, we have a theory about rearranging the users top-k list and order them according to average. We will expand on this in future work in Section \ref{sec:futurework}.

Baltrunas et al found that the more alike a group is, the more effective the group recommender\cite{Baltrunas:2010:GRR:1864708.1864733}. Likewise for our setup, it is likely that the results can vary depending on the recommender used in the individual recommendations stage. It follows that biases introduced by the individual recommendation can result in an either more or less alike-thinking group for the same dataset.

%All methods saw similar decreases in effectiveness across group sizes.

%BC performed best for nDCG. Other benefits include fast computation time, simple to implement. Outperforms or stands equal to various extensions we've developed. Compared with MC for nDCG, the mean difference is small, but consistent.

%MC performed 2nd best for nDCG. Our implementation is very slow, but it can be optimized a lot (probably still not faster than BC). No indication in data, but might surprise for bigger topK lists, which in turn raises the connectivity (main strength). Runs on the Copeland method, which is ill regarded for aggregation, but works reasonably well here.

%Avg generally performs poorly. It gets some good results in Adjusted nDCG, but that is a measurement uniquely advantageous for it among the methods, so it makes little sense as a measure.

%X method is the best

%When to use kendall and nDCG? For our results, it's not that important.